{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Bert for text Classification\n",
    "## ECHR Violation Prediction\n",
    "Following the tutorial of [huggingface](https://huggingface.co/docs/transformers/tasks/sequence_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.utils import load_ECHR, load_ECHR_small\n",
    "\n",
    "# load train, dev and test dataset from json to pandas dataframe\n",
    "df_train, df_dev, df_test = load_ECHR_small()\n",
    "\n",
    "# save to csv\n",
    "#df_train.to_csv('ECHR_Dataset_Small/EN_train_small.csv', index=False)\n",
    "#df_dev.to_csv('ECHR_Dataset_Small/EN_dev_small.csv', index=False)\n",
    "#df_test.to_csv('ECHR_Dataset_Small/EN_test_small.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from csv file\n",
    "#import pandas as pd\n",
    "#df_train = pd.read_csv('ECHR_Dataset_Small/EN_train_small.csv')\n",
    "#df_test = pd.read_csv('ECHR_Dataset_Small/EN_test_small.csv')\n",
    "#df_dev = pd.read_csv('ECHR_Dataset_Small/EN_dev_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [\"7. On 28 September 1994 the applicant's husb...\n",
       "1     ['8. The applicant was born in 1974 and lives ...\n",
       "2     ['5. The first applicant, Mr Ivan Dvořáček, wa...\n",
       "3     ['4. The applicant was born in 1959 and lives ...\n",
       "4     ['6. The applicant was born in 1946.', '7. On ...\n",
       "                            ...                        \n",
       "95    ['The applicant is a Ukrainian citizen, born i...\n",
       "96    ['6. The applicants were born in 1944, 1946 an...\n",
       "97    ['1. The case was referred to the Court by the...\n",
       "98    ['5. The applicant was born in 1979 and lives ...\n",
       "99    ['4. The applicant was born in 1959 and lives ...\n",
       "Name: TEXT, Length: 100, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['TEXT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  VIOLATED_ARTICLES  LABEL\n",
      "0                []      0\n",
      "1                []      0\n",
      "2        ['2', '6']      1\n",
      "3       ['13', '6']      1\n",
      "4             ['5']      1\n",
      "5       ['13', '6']      1\n",
      "6                []      0\n",
      "7             ['6']      1\n",
      "8        ['5', '6']      1\n",
      "9                []      0\n"
     ]
    }
   ],
   "source": [
    "# add a column with 0/1 labels to the dataframe 0 if VIOLATED_ARTICLE is empty, 1 otherwise\n",
    "df_train['LABEL'] = df_train['VIOLATED_ARTICLES'].apply(lambda x: 0 if x == [] else 1)\n",
    "df_dev['LABEL'] = df_dev['VIOLATED_ARTICLES'].apply(lambda x: 0 if x == [] else 1)\n",
    "df_test['LABEL'] = df_test['VIOLATED_ARTICLES'].apply(lambda x: 0 if x == [] else 1)\n",
    "\n",
    "# print VIOLATED_ARTICLES and labels\n",
    "print(df_train[['VIOLATED_ARTICLES', 'LABEL']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                TEXT  LABEL\n",
      "0  [\"7. On 28 September 1994 the applicant's husb...      0\n",
      "1  ['8. The applicant was born in 1974 and lives ...      0\n",
      "2  ['5. The first applicant, Mr Ivan Dvořáček, wa...      1\n",
      "3  ['4. The applicant was born in 1959 and lives ...      1\n",
      "4  ['6. The applicant was born in 1946.', '7. On ...      1\n"
     ]
    }
   ],
   "source": [
    "# remove all columns except text and label\n",
    "df_train = df_train[['TEXT', 'LABEL']]\n",
    "df_dev = df_dev[['TEXT', 'LABEL']]\n",
    "df_test = df_test[['TEXT', 'LABEL']]\n",
    "\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_values = df_train['TEXT'].values\n",
    "dev_values = df_dev['TEXT'].values\n",
    "test_values = df_test['TEXT'].values\n",
    "\n",
    "df_train['TEXT'] = [\"\".join(x) for x in train_values]\n",
    "df_dev['TEXT'] = [\"\".join(x) for x in dev_values]\n",
    "df_test['TEXT'] = [\"\".join(x) for x in test_values]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [\"7. On 28 September 1994 the applicant's husb...\n",
       "1     ['8. The applicant was born in 1974 and lives ...\n",
       "2     ['5. The first applicant, Mr Ivan Dvořáček, wa...\n",
       "3     ['4. The applicant was born in 1959 and lives ...\n",
       "4     ['6. The applicant was born in 1946.', '7. On ...\n",
       "                            ...                        \n",
       "95    ['The applicant is a Ukrainian citizen, born i...\n",
       "96    ['6. The applicants were born in 1944, 1946 an...\n",
       "97    ['1. The case was referred to the Court by the...\n",
       "98    ['5. The applicant was born in 1979 and lives ...\n",
       "99    ['4. The applicant was born in 1959 and lives ...\n",
       "Name: TEXT, Length: 100, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_train['TEXT'][0])\n",
    "df_train['TEXT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change name TEXT to text and LABEL to label\n",
    "df_train.rename(columns={'TEXT': 'text', 'LABEL': 'label'}, inplace=True)\n",
    "df_dev.rename(columns={'TEXT': 'text', 'LABEL': 'label'}, inplace=True)\n",
    "df_test.rename(columns={'TEXT': 'text', 'LABEL': 'label'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    61\n",
       "0    39\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if 0/1 class is balanced\n",
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    34\n",
       "0    16\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_train from pandas dataframe to huggingface dataset format\n",
    "from datasets import Dataset \n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63145cb67bd64205ad36c5c24f56d6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset_tokenized = train_dataset.map(preprocess_function, batched= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 100\n",
      "})\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset_tokenized)\n",
    "print(train_dataset_tokenized[0]['input_ids'].__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39816accca54a7d8fc13ac37bcaff87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 50\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do the same for df_dev\n",
    "dev_dataset = Dataset.from_pandas(df_dev)\n",
    "dev_dataset_tokenized = dev_dataset.map(preprocess_function, batched=True)\n",
    "dev_dataset_tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate \n",
    "import evaluate \n",
    "import numpy as np\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels \n",
    "id2label = {0: 'negative', 1: 'positive'}\n",
    "label2id = {'negative': 0, 'positive': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", \n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='bert_echr',          # output directory\n",
    "    learning_rate=2e-5,              # learning rate\n",
    "    num_train_epochs=5,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='bert_echr/logs',    # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset_tokenized,         # training dataset\n",
    "    eval_dataset=dev_dataset_tokenized,            # evaluation dataset\n",
    "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lilf4p/.pyenv/versions/3.10.11/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690bec2ecaf341698d2fd63121ca5030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"bert-echr-classification\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
