{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Bert for text Classification\n",
    "## ECHR Violation Prediction\n",
    "Following the tutorial of [huggingface](https://huggingface.co/docs/transformers/tasks/sequence_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.utils import load_ECHR\n",
    "\n",
    "# load train, dev and test dataset from json to pandas dataframe\n",
    "df_train, df_dev, df_test = load_ECHR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     VIOLATED_ARTICLES  LABEL\n",
      "5704                []      0\n",
      "5154           [13, 8]      1\n",
      "805                 []      0\n",
      "168                [6]      1\n",
      "3673               [6]      1\n",
      "2269               [6]      1\n",
      "5172               [6]      1\n",
      "1037                []      0\n",
      "5587                []      0\n",
      "4437                []      0\n"
     ]
    }
   ],
   "source": [
    "# add a column with 0/1 labels to the dataframe 0 if VIOLATED_ARTICLE is empty, 1 otherwise\n",
    "df_train['LABEL'] = df_train['VIOLATED_ARTICLES'].apply(lambda x: 0 if x == [] else 1)\n",
    "df_dev['LABEL'] = df_dev['VIOLATED_ARTICLES'].apply(lambda x: 0 if x == [] else 1)\n",
    "df_test['LABEL'] = df_test['VIOLATED_ARTICLES'].apply(lambda x: 0 if x == [] else 1)\n",
    "\n",
    "# print VIOLATED_ARTICLES and labels\n",
    "print(df_train[['VIOLATED_ARTICLES', 'LABEL']].sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                TEXT  LABEL\n",
      "0  [7. On 28 September 1994 the applicant's husba...      0\n",
      "1  [8. The applicant was born in 1974 and lives i...      0\n",
      "2  [5. The first applicant, Mr Ivan Dvo≈ô√°ƒçek, was...      1\n",
      "3  [4. The applicant was born in 1959 and lives i...      1\n",
      "4  [6. The applicant was born in 1946., 7. On 14 ...      1\n"
     ]
    }
   ],
   "source": [
    "# remove all columns except text and label\n",
    "df_train = df_train[['TEXT', 'LABEL']]\n",
    "df_dev = df_dev[['TEXT', 'LABEL']]\n",
    "df_test = df_test[['TEXT', 'LABEL']]\n",
    "\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_values = df_train['TEXT'].values\n",
    "dev_values = df_dev['TEXT'].values\n",
    "test_values = df_test['TEXT'].values\n",
    "\n",
    "df_train['TEXT'] = [\" \".join(x) for x in train_values]\n",
    "df_dev['TEXT'] = [\" \".join(x) for x in dev_values]\n",
    "df_test['TEXT'] = [\" \".join(x) for x in test_values]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       7. On 28 September 1994 the applicant's husban...\n",
       "1       8. The applicant was born in 1974 and lives in...\n",
       "2       5. The first applicant, Mr Ivan Dvo≈ô√°ƒçek, was ...\n",
       "3       4. The applicant was born in 1959 and lives in...\n",
       "4       6. The applicant was born in 1946. 7. On 14 Au...\n",
       "                              ...                        \n",
       "7095    5. The applicant was born in 1943 and lives in...\n",
       "7096    The applicant, Mr Du≈°an V√°clav√≠k, is a Slovaki...\n",
       "7097    4. The applicant was born in 1976 and is curre...\n",
       "7098    The applicants are relatives. They are all Slo...\n",
       "7099    The applicant, a Dutch national, was born in 1...\n",
       "Name: TEXT, Length: 7100, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_train['text'][0])\n",
    "df_train['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change name TEXT to text and LABEL to label\n",
    "df_train.rename(columns={'TEXT': 'text', 'LABEL': 'label'}, inplace=True)\n",
    "df_dev.rename(columns={'TEXT': 'text', 'LABEL': 'label'}, inplace=True)\n",
    "df_test.rename(columns={'TEXT': 'text', 'LABEL': 'label'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 7100\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_train from pandas dataframe to huggingface dataset format\n",
    "from datasets import Dataset \n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f239150671b4029bd2aba7e7b64b4e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset_tokenized = train_dataset.map(preprocess_function, batched= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 7100\n",
      "})\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset_tokenized)\n",
    "print(train_dataset_tokenized[0]['input_ids'].__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce581d5f3994e3fa0741a4056926e2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1380 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 1380\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do the same for df_dev\n",
    "dev_dataset = Dataset.from_pandas(df_dev)\n",
    "dev_dataset_tokenized = dev_dataset.map(preprocess_function, batched=True)\n",
    "dev_dataset_tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate \n",
    "import evaluate \n",
    "import numpy as np\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels \n",
    "id2label = {0: 'negative', 1: 'positive'}\n",
    "label2id = {'negative': 0, 'positive': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", \n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='bert_echr',          # output directory\n",
    "    learning_rate=2e-5,              # learning rate\n",
    "    num_train_epochs=5,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='bert_echr/logs',    # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ü§ó Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset_tokenized,         # training dataset\n",
    "    eval_dataset=dev_dataset_tokenized,            # evaluation dataset\n",
    "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('text-classification', model='bert_echr')\n",
    "\n",
    "classifier(df_test['text'][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
