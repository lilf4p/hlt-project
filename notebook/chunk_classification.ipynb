{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legal Judgement Predictor - A classification task on BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "path_to_datasets = 'embeddings_datasets/legal-bert-base-uncased/'\n",
    "create_dataset = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECHRDataset(Dataset):\n",
    "        def __init__(self, data, attention_mask, labels):\n",
    "            self.data = data\n",
    "            self.attention_mask = attention_mask\n",
    "            self.labels = labels\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.data[idx], self.attention_mask[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if create_dataset:\n",
    "        \n",
    "    # pad the data to be of the same shape\n",
    "    def pad_data(data, max_len):\n",
    "        padded_data = []\n",
    "        attention_masks = []\n",
    "        for i in range(len(data)):\n",
    "            attention_masks.append([1] * data[i].shape[0] + [0] * (max_len - data[i].shape[0]))\n",
    "            padded_data.append(F.pad(data[i], (0, 0, 0, max_len - data[i].shape[0])))\n",
    "        #print(len(attention_masks))\n",
    "        return torch.stack(padded_data), torch.tensor(attention_masks)\n",
    "\n",
    "    # load data\n",
    "    train = torch.load('embeddings/legal-bert-base-uncased/emb_tr_cpu.pkl')\n",
    "    dev = torch.load('embeddings/legal-bert-base-uncased/emb_dev_cpu.pkl')\n",
    "    test = torch.load('embeddings/legal-bert-base-uncased/emb_test_cpu.pkl')\n",
    "\n",
    "    print('Train '+str(len(train)),'Dev '+str(len(dev)), 'Test '+str(len(test)))\n",
    "    \n",
    "    # concat dev to train series\n",
    "    train = np.concatenate((train, dev))\n",
    "\n",
    "    print('Train + Dev = '+str(len(train)))\n",
    "\n",
    "    # load labels\n",
    "    train_labels = pd.read_pickle('embeddings/legal-bert-base-uncased/train_labels.pkl')\n",
    "    dev_labels = pd.read_pickle('embeddings/legal-bert-base-uncased/dev_labels.pkl')\n",
    "    test_labels = pd.read_pickle('embeddings/legal-bert-base-uncased/test_labels.pkl')\n",
    "\n",
    "    # concat dev labels to train labels\n",
    "    train_labels = torch.tensor(np.concatenate((train_labels, dev_labels)))\n",
    "\n",
    "    # pad the data\n",
    "    max_len_train = max([x.shape[0] for x in train])\n",
    "    max_len_test = max([x.shape[0] for x in test])\n",
    "    train, train_attention_masks = pad_data(train, max_len_train)\n",
    "    test, test_attention_masks = pad_data(test, max_len_test)\n",
    "\n",
    "    # create the datasets\n",
    "    train_dataset = ECHRDataset(train, train_attention_masks, train_labels)\n",
    "    test_dataset = ECHRDataset(test, test_attention_masks, test_labels)\n",
    "\n",
    "    print (train_dataset.data.device)\n",
    "\n",
    "    # save the datasets\n",
    "    if not os.path.exists(path_to_datasets):\n",
    "        os.makedirs(path_to_datasets)\n",
    "    torch.save(train_dataset, path_to_datasets+'train_dataset.pt')\n",
    "    torch.save(test_dataset, path_to_datasets+'test_dataset.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8480\n"
     ]
    }
   ],
   "source": [
    "if not create_dataset:\n",
    "    train_dataset = torch.load(path_to_datasets+'train_dataset.pt')\n",
    "    test_dataset = torch.load(path_to_datasets+'test_dataset.pt')\n",
    "    print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # Importing Axes3D for 3D plotting\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "scaler = StandardScaler()\n",
    "mean_train = np.array([x.mean(0).numpy() for x in train_dataset.data])\n",
    "\n",
    "print(mean_train.shape)\n",
    "\n",
    "train_scaled = scaler.fit_transform(mean_train)\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "xpca = pca.fit_transform(train_scaled)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Scatter plot\n",
    "scatter = ax.scatter(xpca[:, 0], xpca[:, 1], xpca[:, 2], c=train_dataset.labels, cmap='viridis')\n",
    "\n",
    "# Adding legend for positive and negative classes\n",
    "plt.legend(handles=[plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=10, label='Negative'),\n",
    "                    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='orange', markersize=10, label='Positive')],\n",
    "           loc='upper right')\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('PC 1')\n",
    "ax.set_ylabel('PC 2')\n",
    "ax.set_zlabel('PC 3')\n",
    "plt.title('PCA of Train Data')\n",
    "\n",
    "# Add colorbar\n",
    "plt.colorbar(scatter, label='Class')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASELINE on avg of chunks\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Assuming you have already defined and scaled mean_train and train_dataset.labels\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_scaled, train_dataset.labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initializing the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Training the classifier\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Calculating metrics\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Random Forest F1:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initializing the SVM classifier\n",
    "svm_classifier = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# Training the classifier\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred_svm = svm_classifier.predict(X_test)\n",
    "\n",
    "# Calculating metrics\n",
    "f1 = f1_score(y_test, y_pred_svm, average='weighted')\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "print(\"SVM F1:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Initializing the MLP classifier\n",
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, random_state=42)\n",
    "\n",
    "# Training the classifier\n",
    "mlp_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred_mlp = mlp_classifier.predict(X_test)\n",
    "\n",
    "# Calculating metrics\n",
    "f1 = f1_score(y_test, y_pred_mlp, average='weighted')\n",
    "print(classification_report(y_test, y_pred_mlp))\n",
    "print(\"MLP F1:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AttentionMLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on MPS\n"
     ]
    }
   ],
   "source": [
    "# check if windows or macos\n",
    "if (torch.cuda.is_available()):\n",
    "    print(\"Running on GPU\")\n",
    "    device = torch.device('cuda')\n",
    "elif (torch.backends.mps.is_available()):\n",
    "    print(\"Running on MPS\")\n",
    "    device = torch.device('mps')\n",
    "else :\n",
    "    print(\"Running on CPU\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "train_dataset.data = train_dataset.data.to(device)\n",
    "train_dataset.labels = train_dataset.labels.to(device)\n",
    "train_dataset.attention_mask = train_dataset.attention_mask.to(device)\n",
    "\n",
    "test_dataset.data = test_dataset.data.to(device)\n",
    "test_dataset.labels = test_dataset.labels.to(device)\n",
    "test_dataset.attention_mask = test_dataset.attention_mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AttentionMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_sizes, dropout=0, weight_decay=0.01):\n",
    "        super(AttentionMLP, self).__init__()\n",
    "        # vector for query attention\n",
    "        self.selector = nn.parameter.Parameter(torch.randn(input_dim, 1))\n",
    "        self.Value= nn.Linear(input_dim, input_dim, bias=False)\n",
    "        self.Key = nn.Linear(input_dim, input_dim, bias=False)\n",
    "        # mlp layers\n",
    "        layers = []\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i + 1]))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        self.output = nn.Linear(hidden_sizes[-1], 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # attention\n",
    "        key = self.Key(x)\n",
    "\n",
    "        value = self.Value(x)\n",
    "\n",
    "        non_normalized_attention = torch.matmul(key, self.selector)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask=attention_mask.unsqueeze(2)\n",
    "\n",
    "            non_normalized_attention = non_normalized_attention.masked_fill(attention_mask == 0, -1e9)\n",
    "        attention = F.softmax(non_normalized_attention, dim=1)\n",
    "        # permute the attention to match the shape of the value\n",
    "        attention = attention.permute(0, 2, 1)\n",
    "\n",
    "        x = torch.matmul(attention, value)\n",
    "\n",
    "        # mlp\n",
    "        x = self.mlp(x)\n",
    "        x = self.output(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1/1 [00:03<00:00,  3.42s/epoch, Train Loss=0.654, Val Loss=0.597, Val Acc=0.707, Val F1=0.73]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1/1 [00:02<00:00,  2.52s/epoch, Train Loss=0.562, Val Loss=0.539, Val Acc=0.74, Val F1=0.767]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1/1 [00:02<00:00,  2.53s/epoch, Train Loss=0.505, Val Loss=0.487, Val Acc=0.766, Val F1=0.768]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1/1 [00:02<00:00,  2.57s/epoch, Train Loss=0.469, Val Loss=0.432, Val Acc=0.803, Val F1=0.814]\n"
     ]
    }
   ],
   "source": [
    "from src.utils import k_fold_attention\n",
    "model = AttentionMLP(768, [768,16])\n",
    "model = model.to(device)\n",
    "results = k_fold_attention ( model, \n",
    "                  criterion=nn.BCELoss(), \n",
    "                  optimizer=optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.001),\n",
    "                  train_dataset= train_dataset, \n",
    "                  k_folds=4, \n",
    "                  epochs=30, \n",
    "                  batch_size=64 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss:  0.5138415475102032\n",
      "avg val acc:  0.7540094339622642\n",
      "avg val f1:  0.7698742748198724\n"
     ]
    }
   ],
   "source": [
    "# print mean val loss and acc over fold \n",
    "loss = 0\n",
    "acc = 0\n",
    "f1 = 0\n",
    "for key,value in results.items():\n",
    "    loss += value['result']['best_val_loss']\n",
    "    acc += value['result']['best_val_acc']\n",
    "    f1 += value['result']['best_val_f1']\n",
    "\n",
    "print('avg val loss: ', loss/len(results))\n",
    "print('avg val acc: ', acc/len(results))\n",
    "print('avg val f1: ', f1/len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print training and validation loss\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# plot two image by side\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].plot(results[2]['stats']['train_losses'], label='Training loss')\n",
    "ax[0].plot(results[2]['stats']['val_losses'], label='Validation loss')\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[1].plot(results[2]['stats']['val_accs'], label='Validation accuracy')\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RnnMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add RELU layer\n",
    "\n",
    "train_dataset.attention_mask = train_dataset.attention_mask.to('cpu')\n",
    "\n",
    "class RNNHead(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, hidden_dim_mlp, output_dim, dropout=0., bidirectional=False):\n",
    "        super(RNNHead, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout, bidirectional=bidirectional)\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim_mlp)\n",
    "        self.fc2 = nn.Linear(hidden_dim_mlp, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "        # pack the padded sequence\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        # forward pass through LSTM\n",
    "        out, (h_n, c_n) = self.rnn(x)\n",
    "        # unpack the packed sequence\n",
    "        output= self.fc(out)\n",
    "        output=self.relu(output)\n",
    "        output = self.fc2(output)\n",
    "        output=self.sigmoid(output)\n",
    "        return output.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import k_fold_rnn\n",
    "# create the model\n",
    "model = RNNHead(768, 768, 2, 16, 1)\n",
    "model = model.to(device)\n",
    "results2 = k_fold_rnn(model, \n",
    "            criterion=nn.BCELoss(), \n",
    "            optimizer=optim.Adam(model.parameters(), lr=0.005, weight_decay=1e-5), \n",
    "            train_dataset=train_dataset, \n",
    "            k_folds=4, \n",
    "            epochs=20, \n",
    "            batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print mean val loss and acc over fold \n",
    "loss = 0\n",
    "acc = 0\n",
    "f1 = 0\n",
    "for key,value in results.items():\n",
    "    loss += value['result']['best_val_loss']\n",
    "    acc += value['result']['best_val_acc']\n",
    "    f1 += value['result']['best_val_f1']\n",
    "\n",
    "print('avg val loss: ', loss/len(results))\n",
    "print('avg val acc: ', acc/len(results))\n",
    "print('avg val f1: ', f1/len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print traianing and validation loss\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# plot two image by side\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].plot(results2[1]['stats']['train_losses'], label='Training loss')\n",
    "ax[0].plot(results2[1]['stats']['val_losses'], label='Validation loss')\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[1].plot(results2[1]['stats']['train_accs'], label='Validation accuracy')\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model \n",
    "#from torch import load\n",
    "#model = load('models/attention-mlp/1709484443.4163241-0.3393.pth')\n",
    "#type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test the best model\n",
    "#criterion = nn.BCELoss()\n",
    "#\n",
    "#from sklearn.metrics import classification_report\n",
    "#with torch.no_grad():\n",
    "#    outputs = model(test_dataset.data, test_dataset.attention_mask)\n",
    "#    loss = criterion(outputs, test_dataset.labels.float())\n",
    "#    print(f'Test loss: {loss.item()}')\n",
    "#    print(classification_report(test_dataset.labels.cpu(), (outputs > 0.5).cpu()))\n",
    "#    # confusion matrix\n",
    "#    from sklearn.metrics import confusion_matrix\n",
    "#    import seaborn as sns\n",
    "#    import matplotlib.pyplot as plt\n",
    "#    cm = confusion_matrix(test_dataset.labels.cpu(), (outputs > 0.5).cpu())\n",
    "#    plt.figure(figsize=(10,7))\n",
    "#    sns.heatmap(cm, annot=True)\n",
    "#    plt.xlabel('Predicted')\n",
    "#    plt.ylabel('True')\n",
    "#    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
