{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "# load data\n",
    "train = torch.load('../embeddings/legal-bert-base-uncased/emb_tr_cpu.pkl')\n",
    "test = torch.load('../embeddings/legal-bert-base-uncased/emb_test_cpu.pkl')\n",
    "dev = torch.load('../embeddings/legal-bert-base-uncased/emb_dev_cpu.pkl')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "# load labels\n",
    "train_labels = pd.read_pickle('../ECHR_Dataset_Tokenized/legal-bert-base-uncased/df_train_tokenized.pkl')['label']\n",
    "test_labels = pd.read_pickle('../ECHR_Dataset_Tokenized/legal-bert-base-uncased/df_test_tokenized.pkl')['label']\n",
    "dev_labels = pd.read_pickle('../ECHR_Dataset_Tokenized/legal-bert-base-uncased/df_dev_tokenized.pkl')['label']\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "# save labels\n",
    "pd.to_pickle(train_labels, 'train_labels.pkl')\n",
    "pd.to_pickle(test_labels, 'test_labels.pkl')\n",
    "pd.to_pickle(dev_labels, 'dev_labels.pkl')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# train labels to tensor to device\n",
    "train = [x.to(device) for x in train]\n",
    "test = [x.to(device) for x in test]\n",
    "dev = [x.to(device) for x in dev]\n",
    "train_labels = torch.tensor(train_labels.values).to(device)\n",
    "test_labels = torch.tensor(test_labels.values).to(device)\n",
    "dev_labels = torch.tensor(dev_labels.values).to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "# create a torch dataset\n",
    "class ECHRDataset(Dataset):\n",
    "    def __init__(self, data, attention_mask, labels):\n",
    "        self.data = data\n",
    "        self.attention_mask = attention_mask\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.attention_mask[idx], self.labels[idx]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "# pad the data to be of the same shape\n",
    "def pad_data(data, max_len):\n",
    "    padded_data = []\n",
    "    attention_masks = []\n",
    "    for i in range(len(data)):\n",
    "        attention_masks.append([1] * data[i].shape[0] + [0] * (max_len - data[i].shape[0]))\n",
    "        padded_data.append(F.pad(data[i], (0, 0, 0, max_len - data[i].shape[0])))\n",
    "    print(len(attention_masks))\n",
    "    return torch.stack(padded_data), torch.tensor(attention_masks)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7100\n",
      "2998\n",
      "1380\n"
     ]
    }
   ],
   "source": [
    "# pad the data\n",
    "max_len_train = max([x.shape[0] for x in train])\n",
    "max_len_test = max([x.shape[0] for x in test])\n",
    "max_len_dev = max([x.shape[0] for x in dev])\n",
    "train, train_attention_masks = pad_data(train, max_len_train)\n",
    "test, test_attention_masks = pad_data(test, max_len_test)\n",
    "dev, dev_attention_masks = pad_data(dev, max_len_dev)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "# create the datasets\n",
    "train_dataset = ECHRDataset(train, train_attention_masks, train_labels)\n",
    "test_dataset = ECHRDataset(test, test_attention_masks, test_labels)\n",
    "dev_dataset = ECHRDataset(dev, dev_attention_masks, dev_labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[-0.1817,  0.5268, -0.7749,  ...,  0.3693,  0.6814, -0.4771],\n         [-0.5845,  0.3973,  0.9896,  ...,  0.2682,  0.1414, -0.6813],\n         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n         ...,\n         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]),\n tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0]),\n tensor(0))"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "# save the datasets\n",
    "path_to_datasets = '../ds_with_attention_mask/'\n",
    "import os\n",
    "if not os.path.exists(path_to_datasets):\n",
    "    os.makedirs(path_to_datasets)\n",
    "torch.save(train_dataset, path_to_datasets+'train_dataset.pt')\n",
    "torch.save(test_dataset, path_to_datasets+'test_dataset.pt')\n",
    "torch.save(dev_dataset, path_to_datasets+'dev_dataset.pt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "class AttentionMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_sizes, dropout=0, weight_decay=0.01):\n",
    "        super(AttentionMLP, self).__init__()\n",
    "        # vector for query attention\n",
    "        self.selector = nn.parameter.Parameter(torch.randn(input_dim, 1))\n",
    "        self.Value= nn.Linear(input_dim, input_dim, bias=False)\n",
    "        self.Key = nn.Linear(input_dim, input_dim, bias=False)\n",
    "        # mlp layers\n",
    "        layers = []\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i + 1]))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        self.output = nn.Linear(hidden_sizes[-1], 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # attention\n",
    "        key = self.Key(x)\n",
    "\n",
    "        value = self.Value(x)\n",
    "\n",
    "        non_normalized_attention = torch.matmul(key, self.selector)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask=attention_mask.unsqueeze(2)\n",
    "\n",
    "            non_normalized_attention = non_normalized_attention.masked_fill(attention_mask == 0, -1e9)\n",
    "        attention = F.softmax(non_normalized_attention, dim=1)\n",
    "        # permute the attention to match the shape of the value\n",
    "        attention = attention.permute(0, 2, 1)\n",
    "\n",
    "        x = torch.matmul(attention, value)\n",
    "\n",
    "        # mlp\n",
    "        x = self.mlp(x)\n",
    "        x = self.output(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x.squeeze()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = AttentionMLP(768, [768, 256, 64])\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "# create the loss function\n",
    "criterion = nn.BCELoss()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "# create the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "data": {
      "text/plain": "[tensor([[[-0.8513,  0.5029,  0.9999,  ...,  0.3402,  0.1261, -0.1561],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          ...,\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n \n         [[-0.4650,  0.8310, -0.9921,  ...,  0.4154,  0.6513, -0.6376],\n          [-0.6296,  0.7994, -0.8645,  ...,  0.3450,  0.6652, -0.6714],\n          [-0.7047,  0.6992, -0.7457,  ...,  0.2913,  0.6056, -0.7340],\n          ...,\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n \n         [[-0.5858,  0.7784, -0.3366,  ...,  0.2684,  0.5538, -0.4406],\n          [-0.7407,  0.6959,  0.9161,  ...,  0.5980,  0.5773, -0.5289],\n          [-0.7007,  0.7931,  0.1993,  ...,  0.5673,  0.5512, -0.5692],\n          ...,\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n \n         ...,\n \n         [[-0.5857,  0.6488, -0.9242,  ...,  0.4277,  0.5139, -0.5370],\n          [-0.7128,  0.6726,  0.6767,  ...,  0.4430,  0.6249, -0.6610],\n          [-0.6967,  0.4760, -0.9834,  ..., -0.1185,  0.5047, -0.5857],\n          ...,\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n \n         [[-0.3009,  0.7351, -0.5912,  ...,  0.3525,  0.6685, -0.6185],\n          [ 0.0854,  0.7024,  0.1169,  ...,  0.3604,  0.8111, -0.7492],\n          [-0.5963,  0.4055,  0.8410,  ...,  0.3117,  0.0611, -0.3017],\n          ...,\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n \n         [[-0.5650,  0.4093,  0.9850,  ...,  0.4159, -0.0966, -0.6283],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          ...,\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]),\n tensor([[1, 0, 0,  ..., 0, 0, 0],\n         [1, 1, 1,  ..., 0, 0, 0],\n         [1, 1, 1,  ..., 0, 0, 0],\n         ...,\n         [1, 1, 1,  ..., 0, 0, 0],\n         [1, 1, 1,  ..., 0, 0, 0],\n         [1, 0, 0,  ..., 0, 0, 0]]),\n tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,\n         0, 1, 1, 0, 0, 1, 1, 0])]"
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the dataloader for the training set\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "next(iter(train_dataloader))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 102])"
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))[1].shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "222it [01:19,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 0.6327354576405104\n",
      "Validation loss: 0.6937826871871948\n",
      "Validation accuracy: 0.5007246376811594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "222it [01:19,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, loss: 0.6025752737983927\n",
      "Validation loss: 0.6662874221801758\n",
      "Validation accuracy: 0.5963768115942029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29it [00:11,  2.52it/s]\n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x000002B39356D2D0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\HUAWEI\\PycharmProjects\\hlt-project2\\venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[122], line 11\u001B[0m\n\u001B[0;32m      9\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(inputs, att_masks)\n\u001B[0;32m     10\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels\u001B[38;5;241m.\u001B[39mfloat())\n\u001B[1;32m---> 11\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     13\u001B[0m running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32m~\\PycharmProjects\\hlt-project2\\venv\\lib\\site-packages\\torch\\_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    486\u001B[0m     )\n\u001B[1;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\hlt-project2\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    195\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    197\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    198\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    199\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 200\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    202\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "from tqdm import tqdm\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for i, data in tqdm(enumerate(train_dataloader, 0)):\n",
    "        inputs, att_masks, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, att_masks)\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, loss: {running_loss / len(train_dataloader)}')\n",
    "    train_loss.append(running_loss / len(train_dataloader))\n",
    "\n",
    "    # validate the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(dev)\n",
    "        loss = criterion(outputs, dev_labels.float())\n",
    "        print(f'Validation loss: {loss.item()}')\n",
    "        accuracy = ((outputs > 0.5) == dev_labels).sum().item() / len(dev_labels)\n",
    "        print(f'Validation accuracy: {accuracy}')\n",
    "        val_loss.append(loss.item())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print traianing and validation loss\n",
    "plt.plot(train_loss, label='Training loss')\n",
    "plt.plot(val_loss, label='Validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(test)\n",
    "    loss = criterion(outputs, test_labels.float())\n",
    "    print(f'Test loss: {loss.item()}')\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(classification_report(test_labels.cpu().numpy(), (outputs > 0.5).cpu().numpy()))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
