{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legal Judgement Predictor - A classification task on BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "path_to_datasets = 'embeddings_datasets/legal-bert-base-uncased/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from src.echrdataset import ECHRDataset\n",
    "from src.utils import create_dataset, load_dataset\n",
    "\n",
    "#create_dataset(path_to_datasets)\n",
    "\n",
    "train_dataset, test_dataset = load_dataset(path_to_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "scaler = StandardScaler()\n",
    "first_chunk_train = np.array([x[0].numpy() for x in train_dataset.data])\n",
    "first_chunk_test = np.array([x[0].numpy() for x in test_dataset.data])\n",
    "\n",
    "print(first_chunk_train.shape)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "xpca = pca.fit_transform(first_chunk_train)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Scatter plot\n",
    "scatter = plt.scatter(xpca[:, 0], xpca[:, 1], c=train_dataset.labels, cmap='crest')\n",
    "\n",
    "# Get legend handles and labels\n",
    "legend1 = plt.legend(*scatter.legend_elements(), title=\"Classes\", fontsize = 'x-large', title_fontsize = 'x-large')\n",
    "\n",
    "# Add legend to the plot\n",
    "plt.gca().add_artist(legend1)\n",
    "# Get unique class labels\n",
    "unique_labels = np.unique(train_dataset.labels)\n",
    "# Add custom labels for class 0 and class 1\n",
    "for label in unique_labels:\n",
    "    plt.text(xpca[train_dataset.labels == label, 0].mean(), xpca[train_dataset.labels == label, 1].mean(), str(label),\n",
    "             horizontalalignment='center',\n",
    "             verticalalignment='center', \n",
    "             bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "plt.title('PCA of Train Data', fontsize='x-large')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(first_chunk_train, train_dataset.labels)\n",
    "y_pred = rf_classifier.predict(first_chunk_test)\n",
    "f1 = f1_score(test_dataset.labels, y_pred, average='weighted')\n",
    "print(classification_report(test_dataset.labels, y_pred))\n",
    "print(\"Random Forest F1:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_classifier = SVC(kernel='linear', random_state=42)\n",
    "svm_classifier.fit(first_chunk_train, train_dataset.labels)\n",
    "y_pred_svm = svm_classifier.predict(first_chunk_test)\n",
    "f1 = f1_score(test_dataset.labels, y_pred_svm, average='weighted')\n",
    "print(classification_report(test_dataset.labels, y_pred_svm))\n",
    "print(\"SVM F1:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(test_dataset.labels, y_pred_svm)\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='d')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(16,), max_iter=200, learning_rate='adaptive', random_state=42)\n",
    "mlp_classifier.fit(first_chunk_train, train_dataset.labels)\n",
    "y_pred_mlp = mlp_classifier.predict(first_chunk_test)\n",
    "f1 = f1_score(test_dataset.labels, y_pred_mlp, average='weighted')\n",
    "print(classification_report(test_dataset.labels, y_pred_mlp))\n",
    "print(\"MLP F1:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AttentionMLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import get_device\n",
    "device = get_device()\n",
    "\n",
    "train_dataset.data = train_dataset.data.to(device)\n",
    "train_dataset.labels = train_dataset.labels.to(device)\n",
    "train_dataset.attention_mask = train_dataset.attention_mask.to(device)\n",
    "\n",
    "test_dataset.data = test_dataset.data.to(device)\n",
    "test_dataset.labels = test_dataset.labels.to(device)\n",
    "test_dataset.attention_mask = test_dataset.attention_mask.to(device)\n",
    "\n",
    "train_dataset.data.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.attentionmlp import AttentionMLP\n",
    "model = AttentionMLP(768, [768,16])\n",
    "model = model.to(device)\n",
    "results = model.k_fold ( \n",
    "                  criterion=nn.BCELoss(), \n",
    "                  train_dataset=train_dataset, \n",
    "                  lr=0.0001,\n",
    "                  weight_decay=0.001,\n",
    "                  k_folds=4, \n",
    "                  epochs=50, \n",
    "                  batch_size=64 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print mean val loss and acc over fold \n",
    "loss = 0\n",
    "acc = 0\n",
    "f1 = 0\n",
    "for key,value in results.items():\n",
    "    loss += value['result']['best_val_loss']\n",
    "    acc += value['result']['best_val_acc']\n",
    "    f1 += value['result']['best_val_f1']\n",
    "\n",
    "print('avg val loss: ', loss/len(results))\n",
    "print('avg val acc: ', acc/len(results))\n",
    "print('avg val f1: ', f1/len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fold = 1\n",
    "plt.style.use('ggplot')\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].plot(results[fold]['stats']['train_losses'], label='Training loss')\n",
    "ax[0].plot(results[fold]['stats']['val_losses'], label='Validation loss')\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[1].plot(results[fold]['stats']['val_accs'], label='Validation accuracy')\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RnnMLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rnnmlp import RnnMLP\n",
    "model = RnnMLP(768, 768, 2, 16, 1, dropout=0.1)\n",
    "model = model.to(device)\n",
    "results2 = model.k_fold( \n",
    "            criterion=nn.BCELoss(), \n",
    "            train_dataset=train_dataset, \n",
    "            lr=0.001, \n",
    "            weight_decay=1e-4, \n",
    "            k_folds=4, \n",
    "            epochs=50, \n",
    "            batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print mean val loss and acc over fold \n",
    "loss = 0\n",
    "acc = 0\n",
    "f1 = 0\n",
    "for key,value in results2.items():\n",
    "    loss += value['result']['best_val_loss']\n",
    "    acc += value['result']['best_val_acc']\n",
    "    f1 += value['result']['best_val_f1']\n",
    "\n",
    "print('avg val loss: ', loss/len(results))\n",
    "print('avg val acc: ', acc/len(results))\n",
    "print('avg val f1: ', f1/len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 4\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].plot(results2[fold]['stats']['train_losses'], label='Training loss')\n",
    "ax[0].plot(results2[fold]['stats']['val_losses'], label='Validation loss')\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[1].plot(results2[fold]['stats']['val_accs'], label='Validation accuracy')\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the best models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AttentionMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.attentionmlp import AttentionMLP\n",
    "model = AttentionMLP(768, [768,16])\n",
    "model = model.to(device)\n",
    "best_model, loss, acc, f1 = model.train_loop( \n",
    "                criterion=nn.BCELoss(),\n",
    "                train_dataset=train_dataset,\n",
    "                lr=0.0001,\n",
    "                weight_decay=0.001,\n",
    "                epochs=50,\n",
    "                batch_size=64\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model, 'models/attention-mlp.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "best_model = AttentionMLP(768, [768,16])\n",
    "best_model.load_state_dict(torch.load('models/attention-mlp.pt'))\n",
    "best_model = best_model.to(device)\n",
    "\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = []\n",
    "    labels_topredict = []\n",
    "    for inputs, att_masks, labels in test_loader:\n",
    "        outputs = best_model(inputs, att_masks)\n",
    "        predictions.append(torch.round(outputs).to('cpu'))\n",
    "        labels_topredict.append(labels.to('cpu'))\n",
    "\n",
    "    predictions = torch.cat(predictions).numpy()\n",
    "    labels_topredict = torch.cat(labels_topredict).numpy()\n",
    "    test_acc = accuracy_score(labels_topredict, predictions)\n",
    "    test_f1 = f1_score(labels_topredict, predictions, average='weighted')\n",
    "\n",
    "print(classification_report(labels_topredict, predictions))\n",
    "print(f\"Test Accuracy: {test_acc}\")\n",
    "print(f\"Test F1: {test_f1}\")\n",
    "\n",
    "cm = confusion_matrix(labels_topredict, predictions)\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RnnMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rnnmlp import RnnMLP\n",
    "model = RnnMLP(768, 768, 2, 16, 1, dropout=0.1)\n",
    "model = model.to(device)\n",
    "best_model, loss, acc, f1 = model.train_loop( \n",
    "            criterion=nn.BCELoss(), \n",
    "            train_dataset=train_dataset, \n",
    "            lr=0.001, \n",
    "            weight_decay=1e-4,  \n",
    "            epochs=50, \n",
    "            batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model, 'models/rnn-mlp.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "model = RnnMLP(768, 768, 2, 16, 1, dropout=0.1)\n",
    "best_model.load_state_dict(torch.load('models/rnn-mlp.pt'))\n",
    "best_model = best_model.to(device)\n",
    "\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = []\n",
    "    labels_topredict = []\n",
    "    for inputs, att_masks, labels in test_loader:\n",
    "        outputs = best_model(inputs, att_masks)\n",
    "        predictions.append(torch.round(outputs).to('cpu'))\n",
    "        labels_topredict.append(labels.to('cpu'))\n",
    "\n",
    "    predictions = torch.cat(predictions).numpy()\n",
    "    labels_topredict = torch.cat(labels_topredict).numpy()\n",
    "    test_acc = accuracy_score(labels_topredict, predictions)\n",
    "    test_f1 = f1_score(labels_topredict, predictions, average='weighted')\n",
    "\n",
    "print(classification_report(labels_topredict, predictions))\n",
    "print(f\"Test Accuracy: {test_acc}\")\n",
    "print(f\"Test F1: {test_f1}\")\n",
    "\n",
    "cm = confusion_matrix(labels_topredict, predictions)\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
