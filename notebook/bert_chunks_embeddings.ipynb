{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings with Legal Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install torch\n",
    "%pip install transformers\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import  torch\n",
    "import  torch.nn as nn\n",
    "import  torch.optim as optim\n",
    "import  torch.nn.functional as F\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e654f0fdd8346ef82713cb7119a7a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bert = transformers.BertModel.from_pretrained('nlpaueb/legal-bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device ='cuda' if torch.cuda.is_available else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle('../ECHR_Dataset_Tokenized/legal-bert-base-uncased/df_train_tokenized.pkl')\n",
    "df_dev = pd.read_pickle('../ECHR_Dataset_Tokenized/legal-bert-base-uncased/df_dev_tokenized.pkl')\n",
    "df_test = pd.read_pickle('../ECHR_Dataset_Tokenized/legal-bert-base-uncased/df_test_tokenized.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ClassificationHeadWithAttention(nn.Module):\n",
    "    \"\"\"\n",
    "        Classification head with attention mechanism\n",
    "        Takes in input n bert embeddings of size 768 and outputs a binary classification\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=768, hidden_size=1024, output_size=1):\n",
    "        super(ClassificationHeadWithAttention, self).__init__()\n",
    "        # self.positional_encoding = positional_encoding()\n",
    "        self.selector = nn.Parameter(torch.Tensor(input_size,1))\n",
    "        nn.init.normal_(self.selector)\n",
    "        self.keys_matrix = nn.Parameter(torch.Tensor(input_size, input_size))\n",
    "        nn.init.normal_(self.keys_matrix)\n",
    "        self.values_matrix = nn.Parameter(torch.Tensor(input_size, input_size))\n",
    "        nn.init.normal_(self.values_matrix)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x):\n",
    "        # apply positional encoding\n",
    "        # x = positional_encoding(x) TODO implement positional encoding\n",
    "        # apply attention mechanism\n",
    "        values = torch.matmul(x, self.values_matrix)\n",
    "        keys = torch.matmul(x, self.keys_matrix)\n",
    "        attention_weights = F.softmax(torch.matmul(values, self.selector), dim=0)\n",
    "        x= torch.matmul(attention_weights.T, values)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        return x, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train= df_train[['input_ids', 'attention_mask', 'label']]\n",
    "df_dev = df_dev[['input_ids', 'attention_mask', 'label']]\n",
    "df_test = df_test[['input_ids', 'attention_mask', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_one_embedding(document, attention_mask, model, batch_size = 1):\n",
    "    emb=[]\n",
    "    with torch.no_grad():\n",
    "        for s, a in zip(document, attention_mask):\n",
    "            emb.append(model(s,a).pooler_output)\n",
    "    print(emb)\n",
    "    return emb  # assuming is a bert model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train['input_ids']=df_train['input_ids'].apply(lambda x: (torch.stack(x)).squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train['attention_mask']=df_train['attention_mask'].apply(lambda x: (torch.stack(x)).squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test['input_ids']=df_test['input_ids'].apply(lambda x: (torch.stack(x)).squeeze(1))\n",
    "df_test['attention_mask']=df_test['attention_mask'].apply(lambda x: (torch.stack(x)).squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_dev['input_ids']=df_dev['input_ids'].apply(lambda x: (torch.stack(x)).squeeze(1))\n",
    "df_dev['attention_mask']=df_dev['attention_mask'].apply(lambda x: (torch.stack(x)).squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['input_ids'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_embedding_batched(input_ids, attention_mask, model, batch_size = 1):\n",
    "    emb=[]\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(input_ids), batch_size):\n",
    "            input_ids_batch = input_ids[i:i+batch_size].to(device)\n",
    "            attention_mask_batch = attention_mask[i:i+batch_size].to(device)\n",
    "            emb.append(model(input_ids_batch, attention_mask_batch).pooler_output)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "def get_embeddings(model, df:pandas.DataFrame):\n",
    "    # apply the embedding model to the dataframe\n",
    "    emb = df.progress_apply(lambda x: get_embedding_batched(x['input_ids'], x['attention_mask'], model,  10), axis=1)\n",
    "    try:\n",
    "        emb=emb.apply(lambda x: torch.cat(x, dim=0))\n",
    "    except Exception as e:\n",
    "        print('sburreck')\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7100/7100 [22:51<00:00,  5.18it/s] \n"
     ]
    }
   ],
   "source": [
    "emb_tr = get_embeddings(model=bert, df = df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1380/1380 [04:41<00:00,  4.90it/s]\n"
     ]
    }
   ],
   "source": [
    "emb_dev= get_embeddings(model=bert, df = df_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2998/2998 [08:19<00:00,  6.00it/s]\n"
     ]
    }
   ],
   "source": [
    "emb_test = get_embeddings(model=bert, df = df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['input_ids'][0].__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "folder_path = '../embeddings/legal-bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "emb_tr.to_pickle(folder_path+'/emb_tr.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "emb_dev.to_pickle(folder_path+'/emb_dev.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "emb_test.to_pickle(folder_path+'/emb_test.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
