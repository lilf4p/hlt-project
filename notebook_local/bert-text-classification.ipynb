{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Bert for text Classification\n",
    "## ECHR Violation Prediction\n",
    "Following the tutorial of [huggingface](https://huggingface.co/docs/transformers/tasks/sequence_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.utils import load_ECHR, load_ECHR_small, subsampling\n",
    "\n",
    "# load train, dev and test dataset from json to pandas dataframe\n",
    "df_train, df_dev, df_test = load_ECHR('ECHR_Dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_sub = subsampling(df_train, n=50)\n",
    "df_dev_sub = subsampling(df_dev, n=50)\n",
    "df_test_sub = subsampling(df_test, n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "df_train_sub.to_csv('ECHR_Dataset_Sub/EN_train_sub.csv', index=False)\n",
    "df_dev_sub.to_csv('ECHR_Dataset_Sub/EN_dev_sub.csv', index=False)\n",
    "df_test_sub.to_csv('ECHR_Dataset_Sub/EN_test_sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from csv file\n",
    "import pandas as pd\n",
    "df_train_sub = pd.read_csv('ECHR_Dataset_Sub/EN_train_sub.csv')\n",
    "df_test_sub = pd.read_csv('ECHR_Dataset_Sub/EN_test_sub.csv')\n",
    "df_dev_sub = pd.read_csv('ECHR_Dataset_Sub/EN_dev_sub.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     ['1. The applicant, Mr Jusuf Nezirović, is a S...\n",
       "1     ['4. The applicant, Mr İbrahim Acar was born i...\n",
       "2     ['6. The applicant was born in 1977 and lives ...\n",
       "3     ['The applicants, Monika and Sascha Freilinger...\n",
       "4     ['The applicant, Mr Pavel Janata, is a Slovaki...\n",
       "                            ...                        \n",
       "95    ['4. The first applicant was born in 1975 and ...\n",
       "96    ['27. The relevant Articles of the Code of Cri...\n",
       "97    ['4. The applicants were born in 1955, 1953 an...\n",
       "98    ['6. The applicants were born in 1964 and 1963...\n",
       "99    ['5. The applicant was born in 1941 and reside...\n",
       "Name: TEXT, Length: 100, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_sub['TEXT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   VIOLATED_ARTICLES  LABEL\n",
      "0                 []      0\n",
      "1                 []      0\n",
      "2                 []      0\n",
      "3                 []      0\n",
      "4                 []      0\n",
      "..               ...    ...\n",
      "95             ['6']      1\n",
      "96             ['6']      1\n",
      "97             ['6']      1\n",
      "98        ['3', '5']      1\n",
      "99             ['6']      1\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# print VIOLATED_ARTICLES and labels\n",
    "print(df_train_sub[['VIOLATED_ARTICLES', 'LABEL']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                TEXT  LABEL\n",
      "0  ['1. The applicant, Mr Jusuf Nezirović, is a S...      0\n",
      "1  ['4. The applicant, Mr İbrahim Acar was born i...      0\n",
      "2  ['6. The applicant was born in 1977 and lives ...      0\n",
      "3  ['The applicants, Monika and Sascha Freilinger...      0\n",
      "4  ['The applicant, Mr Pavel Janata, is a Slovaki...      0\n"
     ]
    }
   ],
   "source": [
    "# remove all columns except text and label\n",
    "df_train_sub = df_train_sub[['TEXT', 'LABEL']]\n",
    "df_dev_sub = df_dev_sub[['TEXT', 'LABEL']]\n",
    "df_test_sub = df_test_sub[['TEXT', 'LABEL']]\n",
    "\n",
    "print(df_train_sub.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_values = df_train_sub['TEXT'].values\n",
    "dev_values = df_dev_sub['TEXT'].values\n",
    "test_values = df_test_sub['TEXT'].values\n",
    "\n",
    "df_train_sub['TEXT'] = [\"\".join(x) for x in train_values]\n",
    "df_dev_sub['TEXT'] = [\"\".join(x) for x in dev_values]\n",
    "df_test_sub['TEXT'] = [\"\".join(x) for x in test_values]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     ['1. The applicant, Mr Jusuf Nezirović, is a S...\n",
       "1     ['4. The applicant, Mr İbrahim Acar was born i...\n",
       "2     ['6. The applicant was born in 1977 and lives ...\n",
       "3     ['The applicants, Monika and Sascha Freilinger...\n",
       "4     ['The applicant, Mr Pavel Janata, is a Slovaki...\n",
       "                            ...                        \n",
       "95    ['4. The first applicant was born in 1975 and ...\n",
       "96    ['27. The relevant Articles of the Code of Cri...\n",
       "97    ['4. The applicants were born in 1955, 1953 an...\n",
       "98    ['6. The applicants were born in 1964 and 1963...\n",
       "99    ['5. The applicant was born in 1941 and reside...\n",
       "Name: TEXT, Length: 100, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_train_sub['TEXT'][0])\n",
    "df_train_sub['TEXT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change name TEXT to text and LABEL to label\n",
    "df_train_sub.rename(columns={'TEXT': 'text', 'LABEL': 'label'}, inplace=True)\n",
    "df_dev_sub.rename(columns={'TEXT': 'text', 'LABEL': 'label'}, inplace=True)\n",
    "df_test_sub.rename(columns={'TEXT': 'text', 'LABEL': 'label'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    50\n",
       "1    50\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if 0/1 class is balanced\n",
    "df_train_sub['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    50\n",
       "0    50\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_sub['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_train from pandas dataframe to huggingface dataset format\n",
    "from datasets import Dataset \n",
    "train_dataset = Dataset.from_pandas(df_train_sub)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6287ccdf9a8426ba62b844d3f33fc7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset_tokenized = train_dataset.map(preprocess_function, batched= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 100\n",
      "})\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset_tokenized)\n",
    "print(train_dataset_tokenized[0]['input_ids'].__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d15aa20330754898bbd56b2cc78a6ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do the same for df_dev\n",
    "dev_dataset = Dataset.from_pandas(df_dev_sub)\n",
    "dev_dataset_tokenized = dev_dataset.map(preprocess_function, batched=True)\n",
    "dev_dataset_tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate \n",
    "import evaluate \n",
    "import numpy as np\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels \n",
    "id2label = {0: 'negative', 1: 'positive'}\n",
    "label2id = {'negative': 0, 'positive': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", \n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='bert_echr',          # output directory\n",
    "    learning_rate=2e-5,              # learning rate\n",
    "    num_train_epochs=5,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='bert_echr/logs',    # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset_tokenized,         # training dataset\n",
    "    eval_dataset=dev_dataset_tokenized,            # evaluation dataset\n",
    "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_tail_tokens(dataset, head:int, tail:int):\n",
    "    \n",
    "    # from dataset transformers to dataframe pandas\n",
    "    df = pd.DataFrame(dataset)\n",
    "\n",
    "    # slice each input_ids value  only if head + tail < len(input_ids)\n",
    "    df['input_ids'] = df['input_ids'].apply(lambda x: x[:head] + x[-tail:] if len(x) > head + tail else x)\n",
    "    # do the same for attention_mask\n",
    "    df['attention_mask'] = df['attention_mask'].apply(lambda x: x[:head] + x[-tail:] if len(x) > head + tail else x)\n",
    "    # do the same for token_type_ids\n",
    "    df['token_type_ids'] = df['token_type_ids'].apply(lambda x: x[:head] + x[-tail:] if len(x) > head + tail else x)\n",
    "    # do the same for labels\n",
    "\n",
    "    # convert back to dataset transformers\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294\n"
     ]
    }
   ],
   "source": [
    "prova = head_tail_tokens(train_dataset_tokenized, 250,250)\n",
    "print(prova[1]['input_ids'].__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print((prova[1]['token_type_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"bert-echr-classification\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
